{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import random\n",
    "from keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import LeakyReLU\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"data/fullTrain.csv\", low_memory=False)\n",
    "val_df = pd.read_csv(\"data/fullValid.csv\", low_memory=False)\n",
    "te_df = pd.read_csv(\"data/fullTest.csv\", low_memory=False)\n",
    "item_df = pd.read_csv(\"data/full_business.csv\", low_memory=False)\n",
    "user_df = pd.read_json(\"data/user.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'index', 'item_address', 'item_categories', 'item_city',\n",
       "       'item_hours', 'item_is_open', 'item_latitude', 'item_longitude',\n",
       "       'item_name', 'item_postal_code', 'item_review_count', 'item_stars',\n",
       "       'item_state', 'stars', 'user_average_stars', 'user_compliment_cool',\n",
       "       'user_compliment_cute', 'user_compliment_funny', 'user_compliment_hot',\n",
       "       'user_compliment_list', 'user_compliment_more', 'user_compliment_note',\n",
       "       'user_compliment_photos', 'user_compliment_plain',\n",
       "       'user_compliment_profile', 'user_compliment_writer', 'user_cool',\n",
       "       'user_elite', 'user_fans', 'user_funny', 'user_id', 'user_name',\n",
       "       'user_review_count', 'user_useful', 'user_yelping_since', 'Sunday_Open',\n",
       "       'Sunday_Close', 'Monday_Open', 'Monday_Close', 'Tuesday_Open',\n",
       "       'Tuesday_Close', 'Wednesday_Open', 'Wednesday_Close', 'Thursday_Open',\n",
       "       'Thursday_Close', 'Friday_Open', 'Friday_Close', 'Saturday_Open',\n",
       "       'Saturday_Close', 'NoiseLevel', 'RestaurantsAttire',\n",
       "       'RestaurantsTakeOut', 'RestaurantsReservations', 'RestaurantsDelivery',\n",
       "       'Alcohol', 'RestaurantsPriceRange2', 'BikeParking', 'HappyHour',\n",
       "       'OutdoorSeating', 'RestaurantsGoodForGroups', 'HasTV', 'Caters',\n",
       "       'GoodForKids', 'BusinessAcceptsCreditCards', 'WiFi', 'GoodForDancing',\n",
       "       'Smoking', 'RestaurantsTableService', 'Corkage', 'CoatCheck', 'BYOB',\n",
       "       'Parking_Street', 'Parking_Valet', 'Parking_Lot', 'Parking_Garage',\n",
       "       'Parking_Validated'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = ['user_average_stars', 'user_compliment_cool', 'user_compliment_cute', 'user_compliment_funny', 'user_compliment_hot', 'user_compliment_list', 'user_compliment_more', 'user_compliment_note', 'user_compliment_photos', 'user_compliment_plain', 'user_compliment_profile', 'user_compliment_writer', 'user_cool', 'user_fans', 'user_funny', 'user_review_count', 'user_useful', 'item_is_open', 'item_latitude', 'item_longitude', 'item_review_count', 'item_stars']\n",
    "tr_ratings = tr_df.stars.values\n",
    "val_ratings = val_df.stars.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_features(df, continuous_columns):\n",
    "    continuous_features = np.float32(df[continuous_columns].values)\n",
    "    return continuous_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_continuous_features = get_continuous_features(tr_df, continuous_cols)\n",
    "val_continuous_features = get_continuous_features(val_df, continuous_cols)\n",
    "te_continuous_features = get_continuous_features(te_df, continuous_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(tr_continuous_features)\n",
    "tr_continuous_features = scaler.transform(tr_continuous_features)\n",
    "val_continuous_features = scaler.transform(val_continuous_features)\n",
    "te_continuous_features = scaler.transform(te_continuous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_deep_columns = ['NoiseLevel', 'RestaurantsAttire', \"RestaurantsTakeOut\", 'RestaurantsReservations',\n",
    "                      'RestaurantsDelivery', 'Alcohol','RestaurantsPriceRange2', 'BikeParking',\n",
    "                      'HappyHour', 'OutdoorSeating','RestaurantsGoodForGroups',\n",
    "                      'HasTV', 'Caters', 'GoodForKids', 'BusinessAcceptsCreditCards',\n",
    "                      'WiFi', 'GoodForDancing', 'Smoking', 'RestaurantsTableService', 'Corkage', 'CoatCheck', \"BYOB\"]\n",
    "extra_shiz =[\"item_city\", \"item_postal_code\", \"item_state\"]\n",
    "item_deep_vocab_lens = []\n",
    "for col_name in item_deep_columns:\n",
    "    tmp = [item for item in tr_df[col_name].unique() if not item!=item]\n",
    "    vocab = dict(zip(tmp, range(1, len(tmp) + 1)))\n",
    "    item_deep_vocab_lens.append(len(vocab) + 1)\n",
    "    item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x] if x in vocab else 0)\n",
    "item_deep_idx_columns = [t + \"_idx\" for t in item_deep_columns]\n",
    "item_to_deep_features = dict(zip(item_df.business_id.values, item_df[item_deep_idx_columns].values.tolist()))\n",
    "tr_deep_features = np.array(tr_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
    "val_deep_features = np.array(val_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
    "te_deep_features = np.array(te_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3, 3, 4, 5, 5, 4, 3, 3, 4, 3, 4, 4, 4, 5, 3, 5, 3, 3, 4, 3]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_deep_vocab_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare wide features...\n",
      "776\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepare wide features...\")\n",
    "#   Prepare binary encoding for each selected categories\n",
    "all_categories = [category for category_list in item_df.item_categories.values for category in category_list.split(\", \")]\n",
    "category_sorted = sorted(Counter(all_categories).items(), key=lambda x: x[1], reverse=True)\n",
    "print(len(category_sorted))\n",
    "selected_categories = [t[0] for t in category_sorted[:500]]\n",
    "print(len(selected_categories))\n",
    "selected_categories_to_idx = dict(zip(selected_categories, range(1, len(selected_categories) + 1)))\n",
    "selected_categories_to_idx['unk'] = 0\n",
    "idx_to_selected_categories = {val: key for key, val in selected_categories_to_idx.items()}\n",
    "#built a dictionary of wide features, of size 500, which were randmoly selected essentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_p_combinations(df, comb_p, topk, output_freq=False):\n",
    "    def get_category_combinations(categories_str, comb_p=2):\n",
    "        categories = categories_str.split(', ')\n",
    "        return list(combinations(categories, comb_p))\n",
    "    all_categories_p_combos = df[\"item_categories\"].apply(\n",
    "        lambda x: get_category_combinations(x, comb_p)).values.tolist()\n",
    "    all_categories_p_combos = [tuple(t) for item in all_categories_p_combos for t in item]\n",
    "    tmp = dict(Counter(all_categories_p_combos))\n",
    "    sorted_categories_combinations = list(sorted(tmp.items(), key=lambda x: x[1], reverse=True))\n",
    "    if output_freq:\n",
    "        return sorted_categories_combinations[:topk]\n",
    "    else:\n",
    "        return [t[0] for t in sorted_categories_combinations[:topk]]\n",
    "\n",
    "\n",
    "def get_wide_features(df):\n",
    "    def categories_to_binary_output(categories):\n",
    "        binary_output = [0 for _ in range(len(selected_categories_to_idx))]\n",
    "        for category in categories.split(', '):\n",
    "            if category in selected_categories_to_idx:\n",
    "                binary_output[selected_categories_to_idx[category]] = 1\n",
    "            else:\n",
    "                binary_output[0] = 1\n",
    "        return binary_output\n",
    "    def categories_cross_transformation(categories):\n",
    "        current_category_set = set(categories.split(', '))\n",
    "        corss_transform_output = [0 for _ in range(len(top_combinations))]\n",
    "        for k, comb_k in enumerate(top_combinations):\n",
    "            if len(current_category_set & comb_k) == len(comb_k):\n",
    "                corss_transform_output[k] = 1\n",
    "            else:\n",
    "                corss_transform_output[k] = 0\n",
    "        return corss_transform_output\n",
    "\n",
    "    category_binary_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_to_binary_output(x)).values.tolist())\n",
    "    category_corss_transform_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_cross_transformation(x)).values.tolist())\n",
    "    return np.concatenate((category_binary_features, category_corss_transform_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_combinations = []\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 2, 70, output_freq=False)\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 3, 20, output_freq=False)\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 4, 10, output_freq=False)\n",
    "top_combinations = [set(t) for t in top_combinations]\n",
    "\n",
    "tr_wide_features = get_wide_features(tr_df)\n",
    "val_wide_features = get_wide_features(val_df)\n",
    "te_wide_features = get_wide_features(te_df)\n",
    "top_combinations\n",
    "tr_wide_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 601)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_wide_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_features = []\n",
    "tr_features.append(tr_continuous_features.tolist())\n",
    "tr_features += [tr_deep_features[:,i].tolist() for i in range(len(tr_deep_features[0]))]\n",
    "tr_features.append(tr_wide_features.tolist())\n",
    "\n",
    "val_features = []\n",
    "val_features.append(val_continuous_features.tolist())\n",
    "val_features += [val_deep_features[:,i].tolist() for i in range(len(val_deep_features[0]))]\n",
    "val_features.append(val_wide_features.tolist())\n",
    "te_features = []\n",
    "te_features.append(te_continuous_features.tolist())\n",
    "te_features += [te_deep_features[:,i].tolist() for i in range(len(te_deep_features[0]))]\n",
    "te_features.append(te_wide_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deepwide_model(len_continuous, deep_vocab_lens, len_wide, embed_size):\n",
    "    input_list = []\n",
    "    continuous_input = Input(shape=(len_continuous,), dtype='float32', name='continuous_input')\n",
    "    input_list.append(continuous_input)\n",
    "    emb_list = []\n",
    "    for vocab_size in deep_vocab_lens:\n",
    "        _input = Input(shape=(1,), dtype='int32')\n",
    "        input_list.append(_input)\n",
    "        _emb = Embedding(output_dim=embed_size, input_dim=vocab_size, input_length=1)(_input)\n",
    "        _emb = Reshape((embed_size,))(_emb)\n",
    "        emb_list.append(_emb)\n",
    "\n",
    "    deep_input = Concatenate()(emb_list + [continuous_input])\n",
    "    dense_1 = Dense(1024, activation='relu')(deep_input)\n",
    "    dense_1_dp = Dropout(0.2)(dense_1)\n",
    "#     dense_2 = Dense(512, activation='relu')(dense_1_dp)\n",
    "#     dense_2_dp = Dropout(0.3)(dense_2)\n",
    "#     dense_3 = Dense(128, activation='relu')(dense_2_dp)\n",
    "#     dense_3_dp = Dropout(0.3)(dense_3)\n",
    "#     dense_4 = Dense(32, activation='relu')(dense_1_dp)\n",
    "#     dense_4_dp = Dropout(0.3)(dense_4)\n",
    "    wide_input = Input(shape=(len_wide,), dtype='float32')\n",
    "    input_list.append(wide_input)\n",
    "    print(len_wide)\n",
    "    print(len_continuous)\n",
    "    fc_input = Concatenate()([dense_1_dp, wide_input])\n",
    "    model_output = Dense(1)(fc_input)\n",
    "    model = Model(inputs=input_list,\n",
    "                  outputs=model_output)\n",
    "    return model\n",
    "\n",
    "def rmse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601\n",
      "22\n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "100000/100000 [==============================] - 15s 150us/step - loss: 1.1233 - val_loss: 1.0674\n",
      "Epoch 2/8\n",
      "100000/100000 [==============================] - 13s 131us/step - loss: 1.0771 - val_loss: 1.0630\n",
      "Epoch 3/8\n",
      "100000/100000 [==============================] - 13s 131us/step - loss: 1.0697 - val_loss: 1.0654\n",
      "Epoch 4/8\n",
      "100000/100000 [==============================] - 13s 132us/step - loss: 1.0657 - val_loss: 1.0590\n",
      "Epoch 5/8\n",
      "100000/100000 [==============================] - 13s 132us/step - loss: 1.0644 - val_loss: 1.0621\n",
      "Epoch 6/8\n",
      "100000/100000 [==============================] - 13s 133us/step - loss: 1.0619 - val_loss: 1.0614\n",
      "Epoch 7/8\n",
      "100000/100000 [==============================] - 14s 139us/step - loss: 1.0614 - val_loss: 1.0615\n",
      "Epoch 8/8\n",
      "100000/100000 [==============================] - 15s 149us/step - loss: 1.0595 - val_loss: 1.0595\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adagrad\n",
    "from keras import backend as K\n",
    "\n",
    "deepwide_model = build_deepwide_model(len(tr_continuous_features[0]),\n",
    "    item_deep_vocab_lens, len(tr_wide_features[0]), embed_size=10)\n",
    "\n",
    "deepwide_model.compile(optimizer='adagrad', loss= 'mse')\n",
    "history = deepwide_model.fit(tr_features, tr_ratings, epochs=8, verbose=1, callbacks=[ModelCheckpoint('model3.h5')],  validation_data=(val_features, val_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RMSE:  1.0221772849658843\n",
      "VALID RMSE:  1.029330121718136\n",
      "Writing test predictions to file done.\n"
     ]
    }
   ],
   "source": [
    "STUDENT_ID = \"20466559\"\n",
    "y_pred = deepwide_model.predict(tr_features)\n",
    "print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = deepwide_model.predict(val_features)\n",
    "x = rmse(y_pred, val_ratings)\n",
    "print(\"VALID RMSE: \", rmse(y_pred, val_ratings))\n",
    "y_pred = deepwide_model.predict(te_features)\n",
    "res_df = pd.DataFrame()\n",
    "\n",
    "res_df['pred'] = y_pred[:, 0]\n",
    "res_df.to_csv(\"{}_{}.csv\".format(STUDENT_ID, x), index=False)\n",
    "print(\"Writing test predictions to file done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
