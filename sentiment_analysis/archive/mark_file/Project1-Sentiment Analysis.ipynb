{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Concatenate, LSTM, Conv2D, MaxPool2D\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as TF\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, CuDNNLSTM, Bidirectional, Reshape, CuDNNGRU\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "from attention_with_context import AttentionWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = string.punctuation\n",
    "set(z)\n",
    "punct = [letter for letter in string.punctuation]\n",
    "stop_words = set(stopwords.words('english') + punct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n",
    "    e.g.\n",
    "    Input: 'It is a nice day. I am happy.'\n",
    "    Output: ['it', 'is', 'a', 'nice', 'day', 'i', 'am', 'happy']\n",
    "    '''\n",
    "    tokens = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(data, seq_length, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of words, type: list\n",
    "    :param seq_length: the length of sequences,, type: int\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    return a dense sequence matrix whose elements are indices of words,\n",
    "    '''\n",
    "    data_matrix = np.zeros((len(data), seq_length), dtype=int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for j, word in enumerate(doc):\n",
    "            # YOUR CODE HERE\n",
    "            if j == seq_length:\n",
    "                break\n",
    "            word_idx = vocab_dict.get(word, 1) # 1 means the unknown word\n",
    "            data_matrix[i, j] = word_idx\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    embeddings_index = dict();\n",
    "    with open('data/glove.6B.300d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split();\n",
    "            word = values[0];\n",
    "            coefs = np.asarray(values[1:], dtype='float32');\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def read_data(file_name, input_length, vocab=None):\n",
    "    \"\"\"\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['words'] = df['text'].apply(tokenize)\n",
    "    \n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        for i in range(len(df)):\n",
    "            for word in df.iloc[i]['words']:\n",
    "                vocab.add(word)\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict['<pad>'] = 0 # 0 means the padding signal\n",
    "    vocab_dict['<unk>'] = 1 # 1 means the unknown word\n",
    "    vocab_size = 2\n",
    "    for v in vocab:\n",
    "        vocab_dict[v] = vocab_size\n",
    "        vocab_size += 1\n",
    "\n",
    "    data_matrix = get_sequence(df['words'], input_length, vocab_dict)\n",
    "    stars = df['stars'].apply(int) - 1\n",
    "    return df['review_id'], stars, data_matrix, vocab, vocab_dict\n",
    "# ----------------- End of Helper Functions-----------------\n",
    "\n",
    "\n",
    "def load_data(input_length):\n",
    "    # Load training data and vocab\n",
    "    train_id_list, train_data_label, train_data_matrix, vocab, vocab_dict = read_data(\"data/train.csv\", input_length)\n",
    "    K = max(train_data_label)+1  # labels begin with 0\n",
    "\n",
    "    # Load valid data\n",
    "    valid_id_list, valid_data_label, valid_data_matrix, vocab, vocab_dict = read_data(\"data/valid.csv\", input_length, vocab=vocab)\n",
    "\n",
    "    # Load testing data\n",
    "    test_id_list, _, test_data_matrix, _, _= read_data(\"data/test.csv\", input_length, vocab=vocab)\n",
    "\n",
    "    print(\"Vocabulary Size:\", len(vocab))\n",
    "    print(\"Training Set Size:\", len(train_id_list))\n",
    "    print(\"Validation Set Size:\", len(valid_id_list))\n",
    "    print(\"Test Set Size:\", len(test_id_list))\n",
    "    print(\"Training Set Shape:\", train_data_matrix.shape)\n",
    "    print(\"Validation Set Shape:\", valid_data_matrix.shape)\n",
    "    print(\"Testing Set Shape:\", test_data_matrix.shape)\n",
    "\n",
    "    # Converts a class vector to binary class matrix.\n",
    "    # https://keras.io/utils/#to_categorical\n",
    "    train_data_label = keras.utils.to_categorical(train_data_label, num_classes=K)\n",
    "    valid_data_label = keras.utils.to_categorical(valid_data_label, num_classes=K)\n",
    "    return train_id_list, train_data_matrix, train_data_label, \\\n",
    "        valid_id_list, valid_data_matrix, valid_data_label, \\\n",
    "        test_id_list, test_data_matrix, None, vocab, vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 200\n",
    "embedding_size = 300 #300\n",
    "hidden_size = 100 #200 # 64\n",
    "batch_size = 128\n",
    "dropout_rate = 0.5\n",
    "embedd_dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "total_epoch = 30\n",
    "word_cutoff = 5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 114544\n",
      "Training Set Size: 100000\n",
      "Validation Set Size: 10000\n",
      "Test Set Size: 10000\n",
      "Training Set Shape: (100000, 200)\n",
      "Validation Set Shape: (10000, 200)\n",
      "Testing Set Shape: (10000, 200)\n"
     ]
    }
   ],
   "source": [
    "train_id_list, train_data_matrix, train_data_label, \\\n",
    "        valid_id_list, valid_data_matrix, valid_data_label, \\\n",
    "        test_id_list, test_data_matrix, _, vocab, vocab_dict = load_data(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict\n",
    "len(vocab)\n",
    "N = train_data_matrix.shape[0]\n",
    "K = train_data_label.shape[1]\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "input_size = len(vocab) + 2\n",
    "output_size = K\n",
    "\n",
    "kernel_sizes = [3, 4, 5]\n",
    "padding = 'valid'\n",
    "activation = 'relu'\n",
    "strides = 1\n",
    "pool_size = 2\n",
    "filters = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total absent words are: 57149\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = get_embeddings() \n",
    "embedding_matrix = np.zeros((input_size, EMBEDDING_DIM))\n",
    "absent_words = 0 \n",
    "for word, i in vocab_dict.items():\n",
    "#     if i > MAX_NUM_WORDS:\n",
    "#         continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else: \n",
    "        absent_words +=1 \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "print (\"total absent words are: {}\".format(absent_words))\n",
    "embedding_layer = Embedding(input_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=input_length,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "100000/100000 [==============================] - 101s 1ms/step - loss: 0.9694 - acc: 0.6032 - val_loss: 0.9485 - val_acc: 0.6099\n",
      "Epoch 2/30\n",
      "100000/100000 [==============================] - 100s 996us/step - loss: 0.8510 - acc: 0.6517 - val_loss: 0.8787 - val_acc: 0.6454\n",
      "Epoch 3/30\n",
      "100000/100000 [==============================] - 100s 1ms/step - loss: 0.7865 - acc: 0.6703 - val_loss: 0.7753 - val_acc: 0.6755\n",
      "Epoch 4/30\n",
      "100000/100000 [==============================] - 99s 993us/step - loss: 0.7430 - acc: 0.6902 - val_loss: 0.8309 - val_acc: 0.6639\n",
      "Epoch 5/30\n",
      "100000/100000 [==============================] - 99s 986us/step - loss: 0.7006 - acc: 0.7049 - val_loss: 0.8571 - val_acc: 0.6564\n",
      "Epoch 6/30\n",
      "100000/100000 [==============================] - 98s 984us/step - loss: 0.6578 - acc: 0.7236 - val_loss: 0.8292 - val_acc: 0.6698\n",
      "Epoch 7/30\n",
      "100000/100000 [==============================] - 101s 1ms/step - loss: 0.6111 - acc: 0.7412 - val_loss: 0.8303 - val_acc: 0.6763\n",
      "Epoch 8/30\n",
      "100000/100000 [==============================] - 102s 1ms/step - loss: 0.5620 - acc: 0.7614 - val_loss: 0.8732 - val_acc: 0.6636\n",
      "Epoch 9/30\n",
      " 89472/100000 [=========================>....] - ETA: 10s - loss: 0.5093 - acc: 0.7847"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(input_length,), dtype='float32')\n",
    "# embed_seq = Embedding(input_size, embedding_size, input_length=input_length)(sequence_input)\n",
    "# embed_drop = Dropout(embedd_dropout)(embed_seq)\n",
    "embed_seq = embedding_layer(sequence_input)\n",
    "# conv_blocks = []\n",
    "# for kernel_size in kernel_sizes:\n",
    "#     conv = Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1)(embed_seq)\n",
    "#     maxpooling = MaxPooling1D(pool_size=pool_size)(conv)\n",
    "# #     conv = Conv2D(filters=filters, kernel_size=(kernel_size, embedding_size), padding=padding, activation=activation, strides=(strides, strides))(conv_input)\n",
    "# #     maxpooling = MaxPool2D(pool_size=((input_length-kernel_size)//strides+1, 1))(conv)\n",
    "#     conv_blocks.append(maxpooling)\n",
    "# c = Concatenate()(conv_blocks) if len(kernel_sizes) > 1 else conv_blocks[0]\n",
    "# c_d = Dropout(dropout_rate)(c)\n",
    "#lstm_output, lstm_state, lstm_cstate = \\\n",
    "#Bidirectional(CuDNNLSTM(units=hidden_size, return_state=True), merge_mode='concat')(c_d)\n",
    "lstm_o = CuDNNLSTM(units=embedding_size, return_sequences=True)(embed_seq)\n",
    "lstm_expanded = Lambda(lambda x: TF.expand_dims(x, -1))(lstm_o)\n",
    "    # CNN layers\n",
    "conv_blocks = []\n",
    "for kernel_size in kernel_sizes:\n",
    "        # YOUR CODE HERE\n",
    "    conv = Conv2D(filters=filters, kernel_size=(kernel_size, \\\n",
    "         embedding_size) ,padding=padding, activation=activation, \\\n",
    "         strides=(strides, strides))(lstm_expanded)\n",
    "    maxpooling = MaxPool2D(pool_size=( \\\n",
    "        (input_length-kernel_size)//strides+1, 1))(conv)\n",
    "    faltten = Flatten()(maxpooling)\n",
    "    conv_blocks.append(faltten)\n",
    "c = Concatenate()(conv_blocks) if len(kernel_sizes) > 1 else conv_blocks[0]\n",
    "# c_r = Lambda(lambda x: TF.reshape(x, [-1, filters]))(c)\n",
    "c_d = Dropout(dropout_rate)(c)\n",
    "preds = Dense(K, activation='softmax')(c_d)\n",
    "model = Model(sequence_input, preds)\n",
    "     # SGD optimizer with momentum\n",
    "optimizer = Adam(lr=learning_rate, decay=1e-)\n",
    "     # compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # training\n",
    "model.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size, validation_data=(valid_data_matrix, valid_data_label))\n",
    "     # testing\n",
    "train_score = model.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'.format(train_score[0], train_score[1]))\n",
    "valid_score = model.evaluate(valid_data_matrix, valid_data_label, batch_size=batch_size)\n",
    "print('Validation Loss: {}\\n Validation Accuracy: {}\\n'.format(valid_score[0], valid_score[1]))\n",
    "\n",
    "    # predicting\n",
    "test_pre = model.predict(test_data_matrix, batch_size=batch_size).argmax(axis=-1) + 1\n",
    "sub_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequence_input = Input(shape=(input_length,), dtype='float32')\n",
    "embedded_sequences = Embedding(input_dim=input_size, output_dim=embedding_size, \n",
    "                                            input_length=input_length)(sequence_input)\n",
    "embedd_dropout = Dropout(dropout_rate)(embedded_sequences)\n",
    "word_lstm = Bidirectional(CuDNNGRU(input_size, return_sequences=True))(embedd_dropout)\n",
    "word_dense = TimeDistributed(Dense(hidden, kernel_regularizer=l2_reg))(word_lstm)\n",
    "word_att = AttentionWithContext()(word_dense)\n",
    "preds = Dense(K)(word_att)\n",
    "model = Model(sequence_input, preds)\n",
    "# lstm_orig = CuDNNLSTM(units= hidden_size, return_sequences=True,\n",
    "#                                       recurrent_initializer='glorot_uniform')(embedded_sequences)\n",
    "                                                               \n",
    "# lstm, forward_h, backward_h = Bidirectional(CuDNNLSTM(units= hidden_size, return_sequences=True,\n",
    "#                                       recurrent_initializer='glorot_uniform'))(lstm_orig)\n",
    "\n",
    "# state_h = Concatenate()([forward_h, backward_h])\n",
    "\n",
    "# context_vector, attention_weights = attention(lstm, state_h)\n",
    "\n",
    "# output = keras.layers.Dense(1, activation='sigmoid')(context_vector)\n",
    "\n",
    "# model = keras.Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "# model.add(Dense(K, activation='softmax'))\n",
    "\n",
    "############################ Non Model Layers #################################################\n",
    "\n",
    "\n",
    "# summarize layers\n",
    "# print(model.summary())\n",
    "    # SGD optimizer with momentum\n",
    "optimizer = Adam(lr=learning_rate, decay=1e-6)\n",
    "    # compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # training\n",
    "model.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size)\n",
    "    # testing\n",
    "train_score = model.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'.format(train_score[0], train_score[1]))\n",
    "valid_score = model.evaluate(valid_data_matrix, valid_data_label, batch_size=batch_size)\n",
    "print('Validation Loss: {}\\n Validation Accuracy: {}\\n'.format(valid_score[0], valid_score[1]))\n",
    "    # predicting\n",
    "test_pre = model.predict(test_data_matrix, batch_size=batch_size).argmax(axis=-1) + 1\n",
    "sub_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "lengths = []\n",
    "tokens = []\n",
    "length = 0 \n",
    "for text in df['text']:\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "            length = length + 1 \n",
    "    lengths.append(length)\n",
    "    length = 0 \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>58.093860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>53.804495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>769.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lengths\n",
       "count  100000.000000\n",
       "mean       58.093860\n",
       "std        53.804495\n",
       "min         0.000000\n",
       "25%        24.000000\n",
       "50%        41.000000\n",
       "75%        74.000000\n",
       "max       769.000000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_dict = {'lengths': lengths}\n",
    "lengths_df = pd.DataFrame(data=lengths_dict)\n",
    "lengths_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-36372367269f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lengths' is not defined"
     ]
    }
   ],
   "source": [
    "x = plt.hist(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.6413e+04, 1.7955e+04, 3.9240e+03, 1.0690e+03, 3.7900e+02,\n",
       "        1.8400e+02, 7.0000e+01, 4.0000e+00, 1.0000e+00, 1.0000e+00]),\n",
       " array([  0. ,  76.9, 153.8, 230.7, 307.6, 384.5, 461.4, 538.3, 615.2,\n",
       "        692.1, 769. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame({\"health\": 100}, in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    100\n",
       "Name: health, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   health\n",
       "0     100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
