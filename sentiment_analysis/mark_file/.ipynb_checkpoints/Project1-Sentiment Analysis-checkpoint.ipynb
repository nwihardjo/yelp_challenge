{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Concatenate, LSTM\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, CuDNNLSTM, Bidirectional, Reshape, CuDNNGRU\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "from attention_with_context import AttentionWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = string.punctuation\n",
    "set(z)\n",
    "punct = [letter for letter in string.punctuation]\n",
    "stop_words = set(stopwords.words('english') + punct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n",
    "    e.g.\n",
    "    Input: 'It is a nice day. I am happy.'\n",
    "    Output: ['it', 'is', 'a', 'nice', 'day', 'i', 'am', 'happy']\n",
    "    '''\n",
    "    tokens = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(data, seq_length, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of words, type: list\n",
    "    :param seq_length: the length of sequences,, type: int\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    return a dense sequence matrix whose elements are indices of words,\n",
    "    '''\n",
    "    data_matrix = np.zeros((len(data), seq_length), dtype=int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for j, word in enumerate(doc):\n",
    "            # YOUR CODE HERE\n",
    "            if j == seq_length:\n",
    "                break\n",
    "            word_idx = vocab_dict.get(word, 1) # 1 means the unknown word\n",
    "            data_matrix[i, j] = word_idx\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    embeddings_index = dict();\n",
    "    with open('data/glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split();\n",
    "            word = values[0];\n",
    "            coefs = np.asarray(values[1:], dtype='float32');\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def read_data(file_name, input_length, vocab=None):\n",
    "    \"\"\"\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['words'] = df['text'].apply(tokenize)\n",
    "    \n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        for i in range(len(df)):\n",
    "            for word in df.iloc[i]['words']:\n",
    "                vocab.add(word)\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict['<pad>'] = 0 # 0 means the padding signal\n",
    "    vocab_dict['<unk>'] = 1 # 1 means the unknown word\n",
    "    vocab_size = 2\n",
    "    for v in vocab:\n",
    "        vocab_dict[v] = vocab_size\n",
    "        vocab_size += 1\n",
    "\n",
    "    data_matrix = get_sequence(df['words'], input_length, vocab_dict)\n",
    "    stars = df['stars'].apply(int) - 1\n",
    "    return df['review_id'], stars, data_matrix, vocab, vocab_dict\n",
    "# ----------------- End of Helper Functions-----------------\n",
    "\n",
    "\n",
    "def load_data(input_length):\n",
    "    # Load training data and vocab\n",
    "    train_id_list, train_data_label, train_data_matrix, vocab, vocab_dict = read_data(\"data/train.csv\", input_length)\n",
    "    K = max(train_data_label)+1  # labels begin with 0\n",
    "\n",
    "    # Load valid data\n",
    "    valid_id_list, valid_data_label, valid_data_matrix, vocab, vocab_dict = read_data(\"data/valid.csv\", input_length, vocab=vocab)\n",
    "\n",
    "    # Load testing data\n",
    "    test_id_list, _, test_data_matrix, _, _= read_data(\"data/test.csv\", input_length, vocab=vocab)\n",
    "\n",
    "    print(\"Vocabulary Size:\", len(vocab))\n",
    "    print(\"Training Set Size:\", len(train_id_list))\n",
    "    print(\"Validation Set Size:\", len(valid_id_list))\n",
    "    print(\"Test Set Size:\", len(test_id_list))\n",
    "    print(\"Training Set Shape:\", train_data_matrix.shape)\n",
    "    print(\"Validation Set Shape:\", valid_data_matrix.shape)\n",
    "    print(\"Testing Set Shape:\", test_data_matrix.shape)\n",
    "\n",
    "    # Converts a class vector to binary class matrix.\n",
    "    # https://keras.io/utils/#to_categorical\n",
    "    train_data_label = keras.utils.to_categorical(train_data_label, num_classes=K)\n",
    "    valid_data_label = keras.utils.to_categorical(valid_data_label, num_classes=K)\n",
    "    return train_id_list, train_data_matrix, train_data_label, \\\n",
    "        valid_id_list, valid_data_matrix, valid_data_label, \\\n",
    "        test_id_list, test_data_matrix, None, vocab, vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 200\n",
    "embedding_size = 100 #300\n",
    "hidden_size = 100 #200 # 64\n",
    "batch_size = 128\n",
    "dropout_rate = 0.5\n",
    "embedd_dropout = 0.2 \n",
    "learning_rate = 0.001\n",
    "total_epoch = 30\n",
    "word_cutoff = 5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 114544\n",
      "Training Set Size: 100000\n",
      "Validation Set Size: 10000\n",
      "Test Set Size: 10000\n",
      "Training Set Shape: (100000, 200)\n",
      "Validation Set Shape: (10000, 200)\n",
      "Testing Set Shape: (10000, 200)\n"
     ]
    }
   ],
   "source": [
    "train_id_list, train_data_matrix, train_data_label, \\\n",
    "        valid_id_list, valid_data_matrix, valid_data_label, \\\n",
    "        test_id_list, test_data_matrix, _, vocab, vocab_dict = load_data(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict\n",
    "len(vocab)\n",
    "N = train_data_matrix.shape[0]\n",
    "K = train_data_label.shape[1]\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "input_size = len(vocab) + 2\n",
    "output_size = K\n",
    "\n",
    "kernel_sizes = [3, 4, 5]\n",
    "padding = 'valid'\n",
    "activation = 'relu'\n",
    "strides = 1\n",
    "pool_size = 2\n",
    "filters =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total absent words are: 57149\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = get_embeddings() \n",
    "embedding_matrix = np.zeros((input_size, EMBEDDING_DIM))\n",
    "absent_words = 0 \n",
    "for word, i in vocab_dict.items():\n",
    "#     if i > MAX_NUM_WORDS:\n",
    "#         continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else: \n",
    "        absent_words +=1 \n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "print (\"total absent words are: {}\".format(absent_words))\n",
    "embedding_layer = Embedding(input_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=input_length,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "100000/100000 [==============================] - 18s 177us/step - loss: 2.0874 - acc: 0.4843 - val_loss: 1.4509 - val_acc: 0.4406\n",
      "Epoch 2/30\n",
      "100000/100000 [==============================] - 16s 160us/step - loss: 1.4157 - acc: 0.4463 - val_loss: 1.3891 - val_acc: 0.4596\n",
      "Epoch 3/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.3735 - acc: 0.4751 - val_loss: 1.3608 - val_acc: 0.4766\n",
      "Epoch 4/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.3600 - acc: 0.4811 - val_loss: 1.3548 - val_acc: 0.4878\n",
      "Epoch 5/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.3547 - acc: 0.4845 - val_loss: 1.3463 - val_acc: 0.4895\n",
      "Epoch 6/30\n",
      "100000/100000 [==============================] - 16s 160us/step - loss: 1.3479 - acc: 0.4874 - val_loss: 1.3568 - val_acc: 0.4760\n",
      "Epoch 7/30\n",
      "100000/100000 [==============================] - 16s 161us/step - loss: 1.3330 - acc: 0.5175 - val_loss: 1.3320 - val_acc: 0.6021\n",
      "Epoch 8/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.3184 - acc: 0.6034 - val_loss: 1.3156 - val_acc: 0.6071\n",
      "Epoch 9/30\n",
      "100000/100000 [==============================] - 16s 160us/step - loss: 1.3215 - acc: 0.6019 - val_loss: 1.3155 - val_acc: 0.6089\n",
      "Epoch 10/30\n",
      "100000/100000 [==============================] - 16s 161us/step - loss: 1.3094 - acc: 0.6123 - val_loss: 1.3268 - val_acc: 0.6145\n",
      "Epoch 11/30\n",
      "100000/100000 [==============================] - 16s 161us/step - loss: 1.3054 - acc: 0.6158 - val_loss: 1.3100 - val_acc: 0.6076\n",
      "Epoch 12/30\n",
      "100000/100000 [==============================] - 16s 161us/step - loss: 1.3043 - acc: 0.6181 - val_loss: 1.3108 - val_acc: 0.6074\n",
      "Epoch 13/30\n",
      "100000/100000 [==============================] - 16s 160us/step - loss: 1.2945 - acc: 0.6241 - val_loss: 1.3075 - val_acc: 0.6107\n",
      "Epoch 14/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.2895 - acc: 0.6279 - val_loss: 1.3143 - val_acc: 0.6171\n",
      "Epoch 15/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.2962 - acc: 0.6242 - val_loss: 1.3101 - val_acc: 0.6114\n",
      "Epoch 16/30\n",
      "100000/100000 [==============================] - 16s 164us/step - loss: 1.3258 - acc: 0.6063 - val_loss: 1.3843 - val_acc: 0.5576\n",
      "Epoch 17/30\n",
      "100000/100000 [==============================] - 16s 163us/step - loss: 1.3141 - acc: 0.6019 - val_loss: 1.3239 - val_acc: 0.6037\n",
      "Epoch 18/30\n",
      "100000/100000 [==============================] - 16s 163us/step - loss: 1.2909 - acc: 0.6272 - val_loss: 1.3101 - val_acc: 0.6097\n",
      "Epoch 19/30\n",
      "100000/100000 [==============================] - 16s 163us/step - loss: 1.2817 - acc: 0.6322 - val_loss: 1.3073 - val_acc: 0.6160\n",
      "Epoch 20/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.2753 - acc: 0.6366 - val_loss: 1.3293 - val_acc: 0.5949\n",
      "Epoch 21/30\n",
      "100000/100000 [==============================] - 16s 163us/step - loss: 1.2755 - acc: 0.6370 - val_loss: 1.3095 - val_acc: 0.6175\n",
      "Epoch 22/30\n",
      "100000/100000 [==============================] - 16s 163us/step - loss: 1.2651 - acc: 0.6448 - val_loss: 1.3390 - val_acc: 0.5997\n",
      "Epoch 23/30\n",
      "100000/100000 [==============================] - 16s 163us/step - loss: 1.2605 - acc: 0.6474 - val_loss: 1.3112 - val_acc: 0.6147\n",
      "Epoch 24/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.2546 - acc: 0.6522 - val_loss: 1.3157 - val_acc: 0.6094\n",
      "Epoch 25/30\n",
      "100000/100000 [==============================] - 16s 163us/step - loss: 1.2478 - acc: 0.6576 - val_loss: 1.3251 - val_acc: 0.6065\n",
      "Epoch 26/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.2454 - acc: 0.6598 - val_loss: 1.3260 - val_acc: 0.6064\n",
      "Epoch 27/30\n",
      "100000/100000 [==============================] - 16s 162us/step - loss: 1.2380 - acc: 0.6645 - val_loss: 1.3162 - val_acc: 0.6136\n",
      "Epoch 28/30\n",
      "100000/100000 [==============================] - 16s 160us/step - loss: 1.2346 - acc: 0.6668 - val_loss: 1.3193 - val_acc: 0.6162\n",
      "Epoch 29/30\n",
      "100000/100000 [==============================] - 16s 160us/step - loss: 1.2290 - acc: 0.6713 - val_loss: 1.3246 - val_acc: 0.6106\n",
      "Epoch 30/30\n",
      "100000/100000 [==============================] - 16s 160us/step - loss: 1.2351 - acc: 0.6677 - val_loss: 1.3277 - val_acc: 0.6093\n",
      "100000/100000 [==============================] - 7s 67us/step\n",
      "Training Loss: 1.2393056494903565\n",
      " Training Accuracy: 0.65326\n",
      "\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "Validation Loss: 1.327671732902527\n",
      " Validation Accuracy: 0.6093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(input_length,), dtype='float32')\n",
    "#embed_seq = Embedding(input_size, embedding_size, input_length=input_length)(sequence_input)\n",
    "embed_seq = embedding_layer(sequence_input)\n",
    "# conv_blocks = []\n",
    "# for kernel_size in kernel_sizes:\n",
    "#     conv = Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1)(embed_seq)\n",
    "#     maxpooling = MaxPooling1D(pool_size=pool_size)(conv)\n",
    "# #     conv = Conv2D(filters=filters, kernel_size=(kernel_size, embedding_size), padding=padding, activation=activation, strides=(strides, strides))(conv_input)\n",
    "# #     maxpooling = MaxPool2D(pool_size=((input_length-kernel_size)//strides+1, 1))(conv)\n",
    "#     conv_blocks.append(maxpooling)\n",
    "# c = Concatenate()(conv_blocks) if len(kernel_sizes) > 1 else conv_blocks[0]\n",
    "# c_d = Dropout(dropout_rate)(c)\n",
    "#lstm_output, lstm_state, lstm_cstate = \\\n",
    "#Bidirectional(CuDNNLSTM(units=hidden_size, return_state=True), merge_mode='concat')(c_d)\n",
    "lstm_output = Bidirectional(CuDNNLSTM(units=hidden_size), merge_mode='concat')(embed_seq)\n",
    "e_d = Reshape((input_length, embedding_size, 1))(e_d)\n",
    "    # CNN layers\n",
    "    conv_blocks = []\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # YOUR CODE HERE\n",
    "        conv = Conv2D(filters=filters, kernel_size=(kernel_size, embedding_size), padding=padding, activation=activation, strides=(strides, strides))(e_d)\n",
    "        maxpooling = MaxPool2D(pool_size=((input_length-kernel_size)//strides+1, 1))(conv)\n",
    "        faltten = Flatten()(maxpooling)\n",
    "        conv_blocks.append(faltten)\n",
    "c = Concatenate()(conv_blocks) if len(kernel_sizes) > 1 else conv_blocks[0]\n",
    "c_d = Dropout(dropout_rate)(c)\n",
    "preds = Dense(K, activation='softmax', kernel_regularizer=regularizers.l1(0.1))(lstm_output)\n",
    "model = Model(sequence_input, preds)\n",
    "     # SGD optimizer with momentum\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "     # compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # training\n",
    "model.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size, validation_data=(valid_data_matrix, valid_data_label))\n",
    "     # testing\n",
    "train_score = model.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'.format(train_score[0], train_score[1]))\n",
    "valid_score = model.evaluate(valid_data_matrix, valid_data_label, batch_size=batch_size)\n",
    "print('Validation Loss: {}\\n Validation Accuracy: {}\\n'.format(valid_score[0], valid_score[1]))\n",
    "\n",
    "    # predicting\n",
    "test_pre = model.predict(test_data_matrix, batch_size=batch_size).argmax(axis=-1) + 1\n",
    "sub_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequence_input = Input(shape=(input_length,), dtype='float32')\n",
    "embedded_sequences = Embedding(input_dim=input_size, output_dim=embedding_size, \n",
    "                                            input_length=input_length)(sequence_input)\n",
    "embedd_dropout = Dropout(dropout_rate)(embedded_sequences)\n",
    "word_lstm = Bidirectional(CuDNNGRU(input_size, return_sequences=True))(embedd_dropout)\n",
    "word_dense = TimeDistributed(Dense(hidden, kernel_regularizer=l2_reg))(word_lstm)\n",
    "word_att = AttentionWithContext()(word_dense)\n",
    "preds = Dense(K)(word_att)\n",
    "model = Model(sequence_input, preds)\n",
    "# lstm_orig = CuDNNLSTM(units= hidden_size, return_sequences=True,\n",
    "#                                       recurrent_initializer='glorot_uniform')(embedded_sequences)\n",
    "                                                               \n",
    "# lstm, forward_h, backward_h = Bidirectional(CuDNNLSTM(units= hidden_size, return_sequences=True,\n",
    "#                                       recurrent_initializer='glorot_uniform'))(lstm_orig)\n",
    "\n",
    "# state_h = Concatenate()([forward_h, backward_h])\n",
    "\n",
    "# context_vector, attention_weights = attention(lstm, state_h)\n",
    "\n",
    "# output = keras.layers.Dense(1, activation='sigmoid')(context_vector)\n",
    "\n",
    "# model = keras.Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "# model.add(Dense(K, activation='softmax'))\n",
    "\n",
    "############################ Non Model Layers #################################################\n",
    "\n",
    "\n",
    "# summarize layers\n",
    "# print(model.summary())\n",
    "    # SGD optimizer with momentum\n",
    "optimizer = Adam(lr=learning_rate, decay=1e-6)\n",
    "    # compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    # training\n",
    "model.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size)\n",
    "    # testing\n",
    "train_score = model.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'.format(train_score[0], train_score[1]))\n",
    "valid_score = model.evaluate(valid_data_matrix, valid_data_label, batch_size=batch_size)\n",
    "print('Validation Loss: {}\\n Validation Accuracy: {}\\n'.format(valid_score[0], valid_score[1]))\n",
    "    # predicting\n",
    "test_pre = model.predict(test_data_matrix, batch_size=batch_size).argmax(axis=-1) + 1\n",
    "sub_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "lengths = []\n",
    "tokens = []\n",
    "length = 0 \n",
    "for text in df['text']:\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "            length = length + 1 \n",
    "    lengths.append(length)\n",
    "    length = 0 \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>58.093860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>53.804495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>769.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lengths\n",
       "count  100000.000000\n",
       "mean       58.093860\n",
       "std        53.804495\n",
       "min         0.000000\n",
       "25%        24.000000\n",
       "50%        41.000000\n",
       "75%        74.000000\n",
       "max       769.000000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_dict = {'lengths': lengths}\n",
    "lengths_df = pd.DataFrame(data=lengths_dict)\n",
    "lengths_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-36372367269f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lengths' is not defined"
     ]
    }
   ],
   "source": [
    "x = plt.hist(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.6413e+04, 1.7955e+04, 3.9240e+03, 1.0690e+03, 3.7900e+02,\n",
       "        1.8400e+02, 7.0000e+01, 4.0000e+00, 1.0000e+00, 1.0000e+00]),\n",
       " array([  0. ,  76.9, 153.8, 230.7, 307.6, 384.5, 461.4, 538.3, 615.2,\n",
       "        692.1, 769. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
